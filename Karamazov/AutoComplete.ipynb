{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Complete\n",
    "\n",
    "[**1. Data preprocessing**](#1.-Data-preprocessing)\n",
    "\n",
    "[**2. Developing n-gram based language model**](#2.-Developing-n\\-gram-based-language-model)\n",
    "\n",
    "[**3. Perplexity**](#3.-Perplexity)\n",
    "\n",
    "[**4. Building the auto-complete system**](#4.-Building-the-auto\\-complete-system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 1962720\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg EBook of The Brothers Karamazov by Fyodor\\nDostoyevsky\\n\\n\\n\\nThis ebook is for the use of anyone anywhere in the United States and most\\nother parts of the world at no cost and with almost no restrictions\\nwhatsoever. You may copy it, give it away or re‐use it under the terms of\\nthe '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Last 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  http://www.gutenberg.org\\n\\n\\nThis Web site includes information about Project Gutenberg™, including how\\nto make donations to the Project Gutenberg Literary Archive Foundation,\\nhow to help produce our new ebooks, and how to subscribe to our email\\nnewsletter to hear about new ebooks.\\n\\n\\n\\n\\n\\n\\n***FINIS***'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "with open(\"./src/Karamazov.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    data = f.read()\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:300])\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Last 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[-300:])\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining get_tokenized_data function\n",
    "\n",
    "Making a list of tokenized sentences\n",
    "\n",
    "**Inputs** :  \n",
    "- *data*: a string corresponding to the string we are starting with\n",
    "\n",
    "**Outputs** :  \n",
    "- *tokenized_sentences*: a list of lists of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    \n",
    "    # Spliting data into sentences using \".\" as the delimiter.\n",
    "    sentences = data.split(\".\")\n",
    "    # Removing leading and trailing spaces from each sentence\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    # Droping sentences if they are empty strings.\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    # Spliting each sentence into tokens\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        tokenized_sentences.append(tokenized)\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sky is high.\n",
      "Grass is green\n",
      "Roses are red.\n",
      "\n",
      "I have a pen.\n",
      "  I have an apple. \n",
      "Ah\n",
      "Apple pen.   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['sky', 'is', 'high'],\n",
       " ['grass', 'is', 'green', 'roses', 'are', 'red'],\n",
       " ['i', 'have', 'a', 'pen'],\n",
       " ['i', 'have', 'an', 'apple'],\n",
       " ['ah', 'apple', 'pen']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = \"Sky is high.\\nGrass is green\\nRoses are red.\\n\\nI have a pen.\\n  I have an apple. \\nAh\\nApple pen.   \\n\"\n",
    "print(x)\n",
    "get_tokenized_data(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19856 sentences are split into 15884 train and 3972 test sets\n",
      "First training sample:\n",
      "['when', 'he', 'entered', 'the', 'household', 'of', 'his', 'patron', 'and', 'benefactor', ',', 'yefim', 'petrovitch', 'polenov', ',', 'he', 'gained', 'the', 'hearts', 'of', 'all', 'the', 'family', ',', 'so', 'that', 'they', 'looked', 'on', 'him', 'quite', 'as', 'their', 'own', 'child']\n",
      "First test sample\n",
      "['who', 'knows', ',', 'he', 'may', 'be', 'of', 'use', 'and', 'make', 'his', 'own', 'career', ',', 'too']\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = get_tokenized_data(data)\n",
    "random.seed(1)\n",
    "random.shuffle(tokenized_data)\n",
    "\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]\n",
    "\n",
    "print(\"{} sentences are split into {} train and {} test sets\".format(len(tokenized_data), len(train_data), len(test_data)))\n",
    "print(\"First training sample:\")\n",
    "print(train_data[0])\n",
    "print(\"First test sample\")\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Handling 'Out of Vocabulary' words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining count_words function\n",
    "\n",
    "Counting the number of word appearence in the tokenized sentences.\n",
    "\n",
    "**Inputs** :  \n",
    "- *tokenized_sentences*: a list of lists of tokens as strings\n",
    "\n",
    "**Outputs** :  \n",
    "- *word_counts*: a dictionary that maps word (str) to the frequency (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "        \n",
    "    word_counts = {}\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence:\n",
    "            if not token in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sky': 1,\n",
       " 'is': 2,\n",
       " 'high': 1,\n",
       " '.': 3,\n",
       " 'grass': 1,\n",
       " 'green': 1,\n",
       " 'roses': 1,\n",
       " 'are': 1,\n",
       " 'red': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences = [['sky', 'is', 'high', '.'],\n",
    "                       ['grass', 'is', 'green', '.'],\n",
    "                       ['roses', 'are', 'red', '.']]\n",
    "count_words(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining get_words_with_nplus_frequency function\n",
    "\n",
    "Finding the words that appear N times or more.\n",
    "\n",
    "**Inputs** :  \n",
    "- *tokenized_sentences*: a list of lists of tokens as strings  \n",
    "- *count_threshold*: minimum number of occurrences for a word to be in the closed vocabulary\n",
    "\n",
    "**Outputs** :  \n",
    "- *closed_vocab*: list of words that appear N times or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "\n",
    "    closed_vocab = []\n",
    "    \n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "\n",
    "    for word, cnt in word_counts.items():\n",
    "        if cnt >= count_threshold:\n",
    "            closed_vocab.append(word)\n",
    "    \n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed vocabulary:\n",
      "['is', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [['sky', 'is', 'high', '.'], ['grass', 'is', 'green', '.'], ['roses', 'are', 'red', '.']]\n",
    "tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
    "print(f\"Closed vocabulary:\")\n",
    "print(tmp_closed_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining replace_oov_words_by_unk function\n",
    "\n",
    "Replacing words not in the given vocabulary with \"\\<unk>\" token.\n",
    "\n",
    "**Inputs** :  \n",
    "- *tokenized_sentences*: a list of lists of tokens as strings  \n",
    "- *closed_vocab*: list of strings that we will use  \n",
    "- *unknown_token*: a string representing unknown (out-of-vocabulary) words\n",
    "\n",
    "**Outputs** :  \n",
    "- *replaced_tokenized_sentences*: a list of lists of strings, with words not in the vocabulary replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, closed_vocab, unknown_token=\"<unk>\"):\n",
    "    \n",
    "    # Placing vocabulary into a set for faster search\n",
    "    closed_vocab = set(closed_vocab)\n",
    "    \n",
    "    replaced_tokenized_sentences = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = []\n",
    "        for token in sentence:\n",
    "            if token in closed_vocab:\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        \n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "        \n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:\n",
      "[['dogs', 'run'], ['cats', 'sleep']]\n",
      "tokenized_sentences with less frequent words converted to '<unk>':\n",
      "[['dogs', '<unk>'], ['<unk>', 'sleep']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\n",
    "closed_vocab = [\"dogs\", \"sleep\"]\n",
    "tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, closed_vocab)\n",
    "print(f\"Original sentence:\")\n",
    "print(tokenized_sentences)\n",
    "print(f\"tokenized_sentences with less frequent words converted to '<unk>':\")\n",
    "print(tmp_replaced_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining process_data function\n",
    "\n",
    "Finding tokens that appear at least N times in the training data.  \n",
    "Replacing tokens that appear less than N times by \"\\<unk>\" both for training and test data. \n",
    "\n",
    "**Inputs** :  \n",
    "- *train_data*: a list of lists of tokens as strings  \n",
    "- *test_data*: a list of lists of tokens as strings  \n",
    "- *count_threshold*: minimum number of occurrences for a word to be in the closed vocabulary\n",
    "\n",
    "**Outputs** :  \n",
    "- *train_data_replaced*: training data with low frequent words replaced by \"\\<unk>\"  \n",
    "- *test_data_replaced*: test data with low frequent words replaced by \"\\<unk>\"  \n",
    "- *closed_vocab*: vocabulary of words that appear n times or more in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(train_data, test_data, count_threshold):\n",
    "\n",
    "    closed_vocab = get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "    \n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data, closed_vocab, unknown_token=\"<unk>\")\n",
    "    \n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data, closed_vocab, unknown_token=\"<unk>\")\n",
    "    \n",
    "    return train_data_replaced, test_data_replaced, closed_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_train_repl\n",
      "[['sky', 'is', 'high', '.'], ['grass', 'is', 'green']]\n",
      "\n",
      "tmp_test_repl\n",
      "[['<unk>', '<unk>', '<unk>', '.']]\n",
      "\n",
      "tmp_vocab\n",
      "['sky', 'is', 'high', '.', 'grass', 'green']\n"
     ]
    }
   ],
   "source": [
    "tmp_train = [['sky', 'is', 'high', '.'],['grass', 'is', 'green']]\n",
    "tmp_test = [['roses', 'are', 'red', '.']]\n",
    "tmp_train_repl, tmp_test_repl, tmp_vocab = process_data(tmp_train, tmp_test, count_threshold = 1)\n",
    "\n",
    "print(\"tmp_train_repl\")\n",
    "print(tmp_train_repl)\n",
    "print()\n",
    "print(\"tmp_test_repl\")\n",
    "print(tmp_test_repl)\n",
    "print()\n",
    "print(\"tmp_vocab\")\n",
    "print(tmp_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the train and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First processed training sample:\n",
      "['when', 'he', 'entered', 'the', 'household', 'of', 'his', '<unk>', 'and', 'benefactor', ',', 'yefim', 'petrovitch', 'polenov', ',', 'he', 'gained', 'the', 'hearts', 'of', 'all', 'the', 'family', ',', 'so', 'that', 'they', 'looked', 'on', 'him', 'quite', 'as', 'their', 'own', 'child']\n",
      "\n",
      "First processed test sample:\n",
      "['who', 'knows', ',', 'he', 'may', 'be', 'of', 'use', 'and', 'make', 'his', 'own', 'career', ',', 'too']\n",
      "\n",
      "First 10 vocabulary:\n",
      "['when', 'he', 'entered', 'the', 'household', 'of', 'his', 'and', 'benefactor', ',']\n",
      "\n",
      "Size of vocabulary: 7298\n"
     ]
    }
   ],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, closed_vocab = process_data(train_data, test_data, minimum_freq)\n",
    "print(\"First processed training sample:\")\n",
    "print(train_data_processed[0])\n",
    "print()\n",
    "print(\"First processed test sample:\")\n",
    "print(test_data_processed[0])\n",
    "print()\n",
    "print(\"First 10 vocabulary:\")\n",
    "print(closed_vocab[0:10])\n",
    "print()\n",
    "print(\"Size of vocabulary:\", len(closed_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Developing n-gram based language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining count_n_grams function\n",
    "\n",
    "Counting all n-grams in the data.\n",
    "\n",
    "**Inputs** :  \n",
    "- *data*: a list of lists of tokens as strings  \n",
    "- *n*: number of words in a sequence  \n",
    "- *start_token*: a token to indicate the beginning of the sentence  \n",
    "- *end_token*: a token to indicate the end of the sentence  \n",
    "\n",
    "**Outputs** :  \n",
    "- *n_grams*: a dictionary that maps a tuple of n-words to its frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "\n",
    "    n_grams = {}\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        sentence = tuple(sentence)\n",
    "        for i in range(len(sentence)-n+1):\n",
    "            n_gram = sentence[i:i+n]\n",
    "            if n_gram in n_grams.keys():\n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uni-gram:\n",
      "{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\n",
      "Bi-gram:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "print(\"Uni-gram:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bi-gram:\")\n",
    "print(count_n_grams(sentences, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining estimate_probability function\n",
    "\n",
    "Estimating the probability of a next word using the n-gram counts with k-smoothing.\n",
    "\n",
    "**Inputs** :  \n",
    "- *word*: next word  \n",
    "- *previous_n_gram*: a sequence of words of length n  \n",
    "- *n_gram_counts*: a dictionary of counts of n-grams  \n",
    "- *n_plus1_gram_counts*: a dictionary of counts of (n+1)-grams  \n",
    "- *vocabulary_size*: number of words in the vocabulary  \n",
    "- *k*: positive constant, smoothing parameter\n",
    "\n",
    "**Outputs** :  \n",
    "- *probability*: probability of word given the prior 'n' words using the n-gram counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_gram,0)\n",
    "    denominator = previous_n_gram_count + (vocabulary_size*k)\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram,0)\n",
    "    numerator = n_plus1_gram_count + k\n",
    "\n",
    "    probability = numerator/denominator\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "tmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining estimate_probabilities function\n",
    "\n",
    "Looping over all words in vocabulary to calculate probabilities for all possible words.\n",
    "\n",
    "**Inputs** :  \n",
    "- *previous_n_gram*: a sequence of words of length n  \n",
    "- *n_gram_counts*: a dictionary of counts of n-grams  \n",
    "- *n_plus1_gram_counts*: a dictionary of counts of (n+1)-grams  \n",
    "- *vocabulary*: list of words  \n",
    "- *k*: positive constant, smoothing parameter\n",
    "\n",
    "**Outputs** :  \n",
    "- *probabilities*: a dictionary mapping from next words to the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    vocabulary_size = len(vocabulary)\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 0.09090909090909091,\n",
       " 'dog': 0.09090909090909091,\n",
       " 'cat': 0.2727272727272727,\n",
       " 'i': 0.09090909090909091,\n",
       " 'a': 0.09090909090909091,\n",
       " 'like': 0.09090909090909091,\n",
       " 'this': 0.09090909090909091,\n",
       " '<e>': 0.09090909090909091,\n",
       " '<unk>': 0.09090909090909091}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining make_count_matrix function\n",
    "\n",
    "Presenting n-gram counts as count matrix.\n",
    "\n",
    "**Inputs** :  \n",
    "- *n_plus1_gram_counts*: a dictionary of counts of (n+1)-grams  \n",
    "- *vocabulary*: list of words  \n",
    "\n",
    "**Outputs** :  \n",
    "- *count_matrix*: count matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
    "\n",
    "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "    \n",
    "    # obtaining unique n-grams\n",
    "    n_grams = []\n",
    "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        n_grams.append(n_gram)\n",
    "    n_grams = list(set(n_grams))\n",
    "    \n",
    "    # mapping from n-gram to row\n",
    "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}\n",
    "    # mapping from next word to column\n",
    "    col_index = {word:j for j, word in enumerate(vocabulary)}\n",
    "    \n",
    "    nrow = len(n_grams)\n",
    "    ncol = len(vocabulary)\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
    "        n_gram = n_plus1_gram[0:-1]\n",
    "        word = n_plus1_gram[-1]\n",
    "        if word not in vocabulary:\n",
    "            continue\n",
    "        i = row_index[n_gram]\n",
    "        j = col_index[word]\n",
    "        count_matrix[i, j] = count\n",
    "    \n",
    "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
    "    \n",
    "    return count_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is</th>\n",
       "      <th>dog</th>\n",
       "      <th>cat</th>\n",
       "      <th>i</th>\n",
       "      <th>a</th>\n",
       "      <th>like</th>\n",
       "      <th>this</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          is  dog  cat    i    a  like  this  <e>  <unk>\n",
       "(like,)  0.0  0.0  0.0  0.0  2.0   0.0   0.0  0.0    0.0\n",
       "(dog,)   1.0  0.0  0.0  0.0  0.0   0.0   0.0  0.0    0.0\n",
       "(a,)     0.0  0.0  2.0  0.0  0.0   0.0   0.0  0.0    0.0\n",
       "(<s>,)   0.0  0.0  0.0  1.0  0.0   0.0   1.0  0.0    0.0\n",
       "(cat,)   0.0  0.0  0.0  0.0  0.0   0.0   0.0  2.0    0.0\n",
       "(is,)    0.0  0.0  0.0  0.0  0.0   1.0   0.0  0.0    0.0\n",
       "(this,)  0.0  1.0  0.0  0.0  0.0   0.0   0.0  0.0    0.0\n",
       "(i,)     0.0  0.0  0.0  0.0  0.0   1.0   0.0  0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "trigram counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is</th>\n",
       "      <th>dog</th>\n",
       "      <th>cat</th>\n",
       "      <th>i</th>\n",
       "      <th>a</th>\n",
       "      <th>like</th>\n",
       "      <th>this</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              is  dog  cat    i    a  like  this  <e>  <unk>\n",
       "(dog, is)    0.0  0.0  0.0  0.0  0.0   1.0   0.0  0.0    0.0\n",
       "(i, like)    0.0  0.0  0.0  0.0  1.0   0.0   0.0  0.0    0.0\n",
       "(this, dog)  1.0  0.0  0.0  0.0  0.0   0.0   0.0  0.0    0.0\n",
       "(<s>, <s>)   0.0  0.0  0.0  1.0  0.0   0.0   1.0  0.0    0.0\n",
       "(is, like)   0.0  0.0  0.0  0.0  1.0   0.0   0.0  0.0    0.0\n",
       "(like, a)    0.0  0.0  2.0  0.0  0.0   0.0   0.0  0.0    0.0\n",
       "(a, cat)     0.0  0.0  0.0  0.0  0.0   0.0   0.0  2.0    0.0\n",
       "(<s>, this)  0.0  1.0  0.0  0.0  0.0   0.0   0.0  0.0    0.0\n",
       "(<s>, i)     0.0  0.0  0.0  0.0  0.0   1.0   0.0  0.0    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "print('bigram counts')\n",
    "display(make_count_matrix(bigram_counts, unique_words))\n",
    "print('\\ntrigram counts')\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "display(make_count_matrix(trigram_counts, unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining make_probability_matrix function\n",
    "\n",
    "Calculating the probabilities of each word given the previous n-gram, and storing this in matrix form.\n",
    "\n",
    "**Inputs** :  \n",
    "- *n_plus1_gram_counts*: a dictionary of counts of (n+1)-grams  \n",
    "- *vocabulary*: list of words  \n",
    "- *k*: positive constant, smoothing parameter\n",
    "\n",
    "**Outputs** :  \n",
    "- *prob_matrix*: probability matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
    "    \n",
    "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
    "    count_matrix += k\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    \n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is</th>\n",
       "      <th>dog</th>\n",
       "      <th>cat</th>\n",
       "      <th>i</th>\n",
       "      <th>a</th>\n",
       "      <th>like</th>\n",
       "      <th>this</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(like,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(dog,)</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(cat,)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i,)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               is       dog       cat         i         a      like      this  \\\n",
       "(like,)  0.090909  0.090909  0.090909  0.090909  0.272727  0.090909  0.090909   \n",
       "(dog,)   0.200000  0.100000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(a,)     0.090909  0.090909  0.272727  0.090909  0.090909  0.090909  0.090909   \n",
       "(<s>,)   0.090909  0.090909  0.090909  0.181818  0.090909  0.090909  0.181818   \n",
       "(cat,)   0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(is,)    0.100000  0.100000  0.100000  0.100000  0.100000  0.200000  0.100000   \n",
       "(this,)  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(i,)     0.100000  0.100000  0.100000  0.100000  0.100000  0.200000  0.100000   \n",
       "\n",
       "              <e>     <unk>  \n",
       "(like,)  0.090909  0.090909  \n",
       "(dog,)   0.100000  0.100000  \n",
       "(a,)     0.090909  0.090909  \n",
       "(<s>,)   0.090909  0.090909  \n",
       "(cat,)   0.272727  0.090909  \n",
       "(is,)    0.100000  0.100000  \n",
       "(this,)  0.100000  0.100000  \n",
       "(i,)     0.100000  0.100000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trigram probabilities\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is</th>\n",
       "      <th>dog</th>\n",
       "      <th>cat</th>\n",
       "      <th>i</th>\n",
       "      <th>a</th>\n",
       "      <th>like</th>\n",
       "      <th>this</th>\n",
       "      <th>&lt;e&gt;</th>\n",
       "      <th>&lt;unk&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(dog, is)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i, like)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(this, dog)</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(is, like)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like, a)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(a, cat)</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, this)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(&lt;s&gt;, i)</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   is       dog       cat         i         a      like  \\\n",
       "(dog, is)    0.100000  0.100000  0.100000  0.100000  0.100000  0.200000   \n",
       "(i, like)    0.100000  0.100000  0.100000  0.100000  0.200000  0.100000   \n",
       "(this, dog)  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n",
       "(<s>, <s>)   0.090909  0.090909  0.090909  0.181818  0.090909  0.090909   \n",
       "(is, like)   0.100000  0.100000  0.100000  0.100000  0.200000  0.100000   \n",
       "(like, a)    0.090909  0.090909  0.272727  0.090909  0.090909  0.090909   \n",
       "(a, cat)     0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n",
       "(<s>, this)  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n",
       "(<s>, i)     0.100000  0.100000  0.100000  0.100000  0.100000  0.200000   \n",
       "\n",
       "                 this       <e>     <unk>  \n",
       "(dog, is)    0.100000  0.100000  0.100000  \n",
       "(i, like)    0.100000  0.100000  0.100000  \n",
       "(this, dog)  0.100000  0.100000  0.100000  \n",
       "(<s>, <s>)   0.181818  0.090909  0.090909  \n",
       "(is, like)   0.100000  0.100000  0.100000  \n",
       "(like, a)    0.090909  0.090909  0.090909  \n",
       "(a, cat)     0.090909  0.272727  0.090909  \n",
       "(<s>, this)  0.100000  0.100000  0.100000  \n",
       "(<s>, i)     0.100000  0.100000  0.100000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "print(\"bigram probabilities\")\n",
    "display(make_probability_matrix(bigram_counts, unique_words, k=1))\n",
    "print(\"trigram probabilities\")\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "display(make_probability_matrix(trigram_counts, unique_words, k=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining calculate_perplexity function\n",
    "\n",
    "Calculating perplexity for a sentence.\n",
    "\n",
    "**Inputs** :  \n",
    "- *sentence*: a list of strings  \n",
    "- *n_gram_counts*: a dictionary of counts of n-grams  \n",
    "- *n_plus1_gram_counts*: a dictionary of counts of (n+1)-grams  \n",
    "- *vocabulary_size*: number of unique words in the vocabulary  \n",
    "- *k*: positive constant, smoothing parameter\n",
    "\n",
    "**Outputs** :  \n",
    "- *perplexity*: perplexity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "\n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    N = len(sentence)\n",
    "\n",
    "    product_pi = 1.0\n",
    "\n",
    "    for t in range(n, N):\n",
    "        n_gram = sentence[t-n:t]\n",
    "        word = sentence[t]\n",
    "        probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n",
    "        product_pi *= (1/probability)\n",
    "\n",
    "    perplexity = product_pi ** (1/N)\n",
    "\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for first train sample: 2.8040\n",
      "Perplexity for test sample: 3.9654\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "perplexity_train1 = calculate_perplexity(sentences[0], unigram_counts, bigram_counts, len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for first train sample: {perplexity_train1:.4f}\")\n",
    "\n",
    "test_sentence = ['i', 'like', 'a', 'dog']\n",
    "perplexity_test = calculate_perplexity(test_sentence, unigram_counts, bigram_counts, len(unique_words), k=1.0)\n",
    "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the auto-complete system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining suggest_a_word function\n",
    "\n",
    "Computing probabilities for all possible next words and suggesting the most likely one.\n",
    "\n",
    "**Inputs** :  \n",
    "- *previous_tokens*: the sentence we input where each token is a word (Must have length > n)  \n",
    "- *n_gram_counts*: a dictionary of counts of n-grams  \n",
    "- *n_plus1_gram_counts*: a dictionary of counts of (n+1)-grams  \n",
    "- *vocabulary*: list of words  \n",
    "- *k*: positive constant, smoothing parameter  \n",
    "- *start_with*: if not None, specifies the first few letters of the next word  \n",
    "\n",
    "**Outputs** :  \n",
    "- *suggestion*: a string of the most likely next word  \n",
    "- *max_prob*: corresponding probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "\n",
    "    probabilities = estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k)\n",
    "    \n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "    \n",
    "    for word, prob in probabilities.items(): \n",
    "        if start_with: \n",
    "            if not word.startswith(start_with): \n",
    "                continue \n",
    "        if prob > max_prob:\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "    \n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like',\n",
      "\tand the suggested word is `a` with a probability of 0.2727\n",
      "\n",
      "The previous words are 'i like', the suggestion must start with `c`\n",
      "\tand the suggested word is `cat` with a probability of 0.0909\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
    "\n",
    "print()\n",
    "tmp_starts_with = 'c'\n",
    "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
    "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining get_suggestions function\n",
    "\n",
    "Looping over varioud n-gram models to get multiple suggestions.\n",
    "\n",
    "**Inputs** :  \n",
    "- *previous_tokens*: the sentence we input where each token is a word (Must have length > n)  \n",
    "- *n_gram_counts_list*: list of n-gram counts each containing a dictionary of counts of n-grams  \n",
    "- *vocabulary*: list of words  \n",
    "- *k*: positive constant, smoothing parameter  \n",
    "- *start_with*: if not None, specifies the first few letters of the next word  \n",
    "\n",
    "**Outputs** :  \n",
    "- *suggestions*: a list of the most likely next words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    \n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    \n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        suggestion = suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=k, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "        \n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are 'i like', the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 0.2727272727272727),\n",
       " ('a', 0.2),\n",
       " ('is', 0.1111111111111111),\n",
       " ('is', 0.1111111111111111)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [['i', 'like', 'a', 'cat'], ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "quadgram_counts = count_n_grams(sentences, 4)\n",
    "qintgram_counts = count_n_grams(sentences, 5)\n",
    "\n",
    "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
    "previous_tokens = [\"i\", \"like\"]\n",
    "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
    "\n",
    "print(f\"The previous words are 'i like', the suggestions are:\")\n",
    "display(tmp_suggest3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the list of n-gram counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suggesting multiple words using n-grams of varying length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['here', 'i', 'am'], the suggestions are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('not', 0.010126582278481013),\n",
       " ('not', 0.010094556606184513),\n",
       " ('again', 0.00027393507738665936),\n",
       " ('when', 0.000136986301369863)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "previous_tokens = [\"here\", \"i\", \"am\"]\n",
    "tmp_suggest = get_suggestions(previous_tokens, n_gram_counts_list, closed_vocab, k=1.0)\n",
    "\n",
    "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
    "display(tmp_suggest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
