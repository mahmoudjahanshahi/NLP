{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a program that translates English to French."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Importing packages and downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The word embeddings data for English and French words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English embeddings from Google code archive word2vec https://code.google.com/archive/p/word2vec/  \n",
    "French embeddings from cross_lingual_text_classification https://github.com/vjstark/crosslingual_text_classification -> https://dl.fbaipublicfiles.com/arrival/vectors/wiki.multi.fr.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings = KeyedVectors.load_word2vec_format('./src/GoogleNews-vectors-negative300.bin', binary = True)\n",
    "fr_embeddings = KeyedVectors.load_word2vec_format('./src/wiki.multi.fr.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the english to french dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the english to french dictionary given a file where each column corresponds to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(file_name):\n",
    "    \n",
    "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
    "    etof = {}  # the english to french dictionary to be returned\n",
    "    for i in range(len(my_file)):\n",
    "        en = my_file.loc[i][0]\n",
    "        fr = my_file.loc[i][1]\n",
    "        etof[en] = fr\n",
    "\n",
    "    return etof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the english to french training dictionary is 5000\n",
      "The length of the english to french test dictionary is 5000\n"
     ]
    }
   ],
   "source": [
    "en_fr_train = get_dict('./data/en-fr.train.txt')\n",
    "print('The length of the english to french training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('./data/en-fr.test.txt')\n",
    "print('The length of the english to french test dictionary is', len(en_fr_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subseting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_set = set(en_embeddings.vocab)\n",
    "french_set = set(fr_embeddings.vocab)\n",
    "en_embeddings_subset = {}\n",
    "fr_embeddings_subset = {}\n",
    "french_words = set(en_fr_train.values())\n",
    "\n",
    "for en_word in en_fr_train.keys():\n",
    "    fr_word = en_fr_train[en_word]\n",
    "    if fr_word in french_set and en_word in english_set:\n",
    "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
    "\n",
    "\n",
    "for en_word in en_fr_test.keys():\n",
    "    fr_word = en_fr_test[en_word]\n",
    "    if fr_word in french_set and en_word in english_set:\n",
    "        en_embeddings_subset[en_word] = en_embeddings[en_word]\n",
    "        fr_embeddings_subset[fr_word] = fr_embeddings[fr_word]\n",
    "\n",
    "\n",
    "pickle.dump( en_embeddings_subset, open( \"./data/en_embeddings.p\", \"wb\" ) )\n",
    "pickle.dump( fr_embeddings_subset, open( \"./data/fr_embeddings.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"./data/en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"./data/fr_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Generate embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining get_matrices function**\n",
    "\n",
    "**Inputs** :  \n",
    "*en_fr*: English to French dictionary  \n",
    "*french_vecs*: French words to their corresponding word embeddings.  \n",
    "*english_vecs*: English words to their corresponding word embeddings.\n",
    "\n",
    "**Outputs** :  \n",
    "*X*: a matrix where the columns are the English embeddings.  \n",
    "*Y*: a matrix where the columns correspong to the French embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, french_vecs, english_vecs):\n",
    "\n",
    "    X_l = list()\n",
    "    Y_l = list()\n",
    "\n",
    "    english_set = set(english_vecs.keys())\n",
    "    french_set = set(french_vecs.keys())\n",
    "\n",
    "    french_words = set(en_fr.values())\n",
    "\n",
    "    for en_word, fr_word in en_fr.items():\n",
    "\n",
    "        if fr_word in french_set and en_word in english_set:\n",
    "            en_vec = english_vecs[en_word]\n",
    "            fr_vec = french_vecs[fr_word]\n",
    "            X_l.append(en_vec)\n",
    "            Y_l.append(fr_vec)\n",
    "\n",
    "    X = np.vstack(X_l)\n",
    "    Y = np.vstack(Y_l)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_matrices(en_fr_train, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Translation as linear transformation of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function will be squared Frobenoius norm of the difference between matrix and its approximation, divided by the number of training examples ùëö ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining compute_loss function**\n",
    "\n",
    "**Inputs** :  \n",
    "*X*: a matrix of dimension (m,n) where the columns are the English embeddings.  \n",
    "*Y*: a matrix of dimension (m,n) where the columns correspong to the French embeddings.  \n",
    "*R*: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "\n",
    "**Outputs** :  \n",
    "*L*: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "\n",
    "    m = np.shape(X)[0]\n",
    "    \n",
    "    diff = np.dot(X,R)-Y\n",
    "    diff_squared = np.square(diff)\n",
    "    sum_diff_squared = np.sum(diff_squared)\n",
    "\n",
    "    loss = sum_diff_squared/m\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the gradient of loss in respect to transform matrix R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining compute_gradient function**\n",
    "\n",
    "**Inputs** :  \n",
    "*X*: a matrix of dimension (m,n) where the columns are the English embeddings.  \n",
    "*Y*: a matrix of dimension (m,n) where the columns correspong to the French embeddings.  \n",
    "*R*: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.\n",
    "\n",
    "**Outputs** :  \n",
    "*gradient*: a matrix of dimension (n,n) - gradient of the loss function L for given X, Y and R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "\n",
    "    m = np.shape(X)[0]\n",
    "\n",
    "    gradient = np.dot(X.T,(np.dot(X,R)-Y)) * (2/m)\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal R with gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining align_embeddings function**\n",
    "\n",
    "**Inputs** :  \n",
    "*X*: a matrix of dimension (m,n) where the columns are the English embeddings.  \n",
    "*Y*: a matrix of dimension (m,n) where the columns correspong to the French embeddings.  \n",
    "*iterations*: positive int - describes how many steps will gradient descent algorithm do.  \n",
    "*learning_rate*: positive float - describes how big steps will  gradient descent algorithm do.\n",
    "\n",
    "**Outputs** :  \n",
    "R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||XR-Y||^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, iterations=100, learning_rate=0.0003):\n",
    "\n",
    "    R = np.random.rand(X.shape[1], X.shape[1])\n",
    "\n",
    "    for i in range(iterations):\n",
    "        if i % 25 == 0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\")\n",
    "\n",
    "        gradient = compute_gradient(X, Y, R)\n",
    "\n",
    "        R -= learning_rate*gradient\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating transformation matrix R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 954.8366\n",
      "loss at iteration 25 is: 97.7061\n",
      "loss at iteration 50 is: 26.8728\n",
      "loss at iteration 75 is: 9.8347\n",
      "loss at iteration 100 is: 4.4102\n",
      "loss at iteration 125 is: 2.3495\n",
      "loss at iteration 150 is: 1.4620\n",
      "loss at iteration 175 is: 1.0429\n",
      "loss at iteration 200 is: 0.8312\n",
      "loss at iteration 225 is: 0.7186\n",
      "loss at iteration 250 is: 0.6562\n",
      "loss at iteration 275 is: 0.6205\n",
      "loss at iteration 300 is: 0.5994\n",
      "loss at iteration 325 is: 0.5867\n",
      "loss at iteration 350 is: 0.5789\n",
      "loss at iteration 375 is: 0.5740\n",
      "loss at iteration 400 is: 0.5709\n",
      "loss at iteration 425 is: 0.5688\n",
      "loss at iteration 450 is: 0.5675\n",
      "loss at iteration 475 is: 0.5666\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, iterations=500, learning_rate=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Testing the translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fining k-Nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining cosine_similarity function**\n",
    "\n",
    "**Inputs** :  \n",
    "*A*: a numpy array which corresponds to a word vector  \n",
    "*B*: A numpy array which corresponds to a word vector\n",
    "\n",
    "**Outputs** :  \n",
    "*cos*: numerical number representing the cosine similarity between A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "\n",
    "    cos = -10\n",
    "    dot = np.dot(A, B)\n",
    "    norma = np.linalg.norm(A)\n",
    "    normb = np.linalg.norm(B)\n",
    "    cos = dot / (norma * normb)\n",
    "\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining nearest_neighbor function**\n",
    "\n",
    "**Inputs** :  \n",
    "*v*: the vector we are going find the nearest neighbor for\n",
    "*candidates*: a set of vectors where we will find the neighbors  \n",
    "*k*: top k nearest neighbors to find\n",
    "\n",
    "**Outputs** :  \n",
    "*k_idx*: the indices of the top k closest vectors in sorted form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1):\n",
    "\n",
    "    similarity_l = []\n",
    "\n",
    "    for row in candidates:\n",
    "        cos_similarity = cosine_similarity(v,row)\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    \n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the translation and computing its accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining test_vocabulary function**\n",
    "\n",
    "**Inputs** :  \n",
    "*X*: a matrix where the columns are the English embeddings.  \n",
    "*Y*: a matrix where the columns correspong to the French embeddings.  \n",
    "*R*: the transform matrix which translates word embeddings from English to French word vector space.\n",
    "\n",
    "**Outputs** :  \n",
    "*accuracy*: for the English to French translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R):\n",
    "\n",
    "    pred = np.dot(X,R)\n",
    "\n",
    "    num_correct = 0\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        pred_idx = nearest_neighbor(pred[i], Y)\n",
    "\n",
    "        if pred_idx == i:\n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = num_correct/np.shape(pred)[0]\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation mechanism working on the unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.552\n"
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train)\n",
    "print(f\"accuracy on test set is {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We managed to translate words from one language to another language without ever seing them with 55% accuracy by using some basic linear algebra and learning a mapping of words from one language to another!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
