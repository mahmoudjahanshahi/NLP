{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH and Document Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a more efficient version of k-nearest neighbors using locality sensitive hashing (LSH) and then applying this to document search. Explicitly:\n",
    "\n",
    "- Processing the tweets and representing each tweet as a vector (representing a document with a vector embedding).\n",
    "- Using locality sensitive hashing and k nearest neighbors to find tweets that are similar to a given tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading twitter samples and stopwords from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Mahmoud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "all_tweets = all_positive_tweets + all_negative_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading english embeddings.  \n",
    "For more details, refer to: https://github.com/mahmoudjs14/NLP/blob/master/word2vec/NaiveMachineTranslation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings_subset = pickle.load(open(\"../word2vec/data/en_embeddings.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining process_tweet function**\n",
    "\n",
    "**Inputs** :  \n",
    "- *tweet*: a string containing a tweet\n",
    "\n",
    "**Output** :  \n",
    "- *tweets_clean*: a list of words containing the processed tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # removing stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # removing old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # removing hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # removing hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # removing stopwords\n",
    "            word not in string.punctuation):  # removing punctuation\n",
    "\n",
    "            stem_word = stemmer.stem(word)\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining cosine_similarity function**\n",
    "\n",
    "**Inputs**:\n",
    "- *A*: a numpy array which corresponds to a word vector\n",
    "- *B*: A numpy array which corresponds to a word vector\n",
    "\n",
    "**Outputs**:\n",
    "- *cos*: numerical number representing the cosine similarity between A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "\n",
    "    cos = -10\n",
    "    dot = np.dot(A, B)\n",
    "    norma = np.linalg.norm(A)\n",
    "    normb = np.linalg.norm(B)\n",
    "    cos = dot / (norma * normb)\n",
    "\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining nearest_neighbor function**\n",
    "\n",
    "**Inputs** :  \n",
    "*v*: the vector we are going find the nearest neighbor for\n",
    "*candidates*: a set of vectors where we will find the neighbors  \n",
    "*k*: top k nearest neighbors to find\n",
    "\n",
    "**Outputs** :  \n",
    "*k_idx*: the indices of the top k closest vectors in sorted form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1):\n",
    "\n",
    "    similarity_l = []\n",
    "\n",
    "    for row in candidates:\n",
    "        cos_similarity = cosine_similarity(v,row)\n",
    "        similarity_l.append(cos_similarity)\n",
    "        \n",
    "    sorted_ids = np.argsort(similarity_l)\n",
    "\n",
    "    k_idx = sorted_ids[-k:]\n",
    "    \n",
    "    return k_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining get_document_embedding function**\n",
    "\n",
    "**Inputs** :  \n",
    "- *tweet*: a string  \n",
    "- *en_embeddings*: a dictionary of word embeddings\n",
    "\n",
    "**Outputs** :  \n",
    "- *doc_embedding*: sum of all word embeddings in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(tweet, en_embeddings): \n",
    "\n",
    "    doc_embedding = np.zeros(300)\n",
    "\n",
    "    processed_doc = process_tweet(tweet)\n",
    "    for word in processed_doc:\n",
    "        doc_embedding += en_embeddings.get(word,0)\n",
    "\n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing all document vectors into a dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining get_document_vecs function**\n",
    "\n",
    "**Inputs** :  \n",
    "- *all_docs*: list of strings - all tweets in our dataset.  \n",
    "- *en_embeddings*: dictionary with words as the keys and their embeddings as the values.\n",
    "\n",
    "**Outputs** :\n",
    "- *document_vec_matrix*: matrix of tweet embeddings.\n",
    "- *ind2Doc_dict*: dictionary with indices of tweets in vecs as keys and their embeddings as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vecs(all_docs, en_embeddings):\n",
    "\n",
    "    ind2Doc_dict = {}\n",
    "\n",
    "    document_vec_l = []\n",
    "\n",
    "    for i, doc in enumerate(all_docs):\n",
    "        doc_embedding = get_document_embedding(doc, en_embeddings)\n",
    "        ind2Doc_dict[i] = doc_embedding\n",
    "        document_vec_l.append(doc_embedding)\n",
    "\n",
    "    document_vec_matrix = np.vstack(document_vec_l)\n",
    "\n",
    "    return document_vec_matrix, ind2Doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary 10000\n",
      "shape of document_vecs (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)\n",
    "print(f\"length of dictionary {len(ind2Tweet)}\")\n",
    "print(f\"shape of document_vecs {document_vecs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Looking up the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputing a tweet, and use cosine similarity to see which tweet in our corpus is similar to this tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweet = 'i like listening to rock music!'\n",
    "process_tweet(my_tweet)\n",
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I honestly had a blast getting shitfaced with the chat and with all you kind souls donating over 70$ tonight, singing disney songs was :D\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\n",
    "print(all_tweets[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finding the most similar tweets with LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing locality sensitive hashing (LSH) to identify the most similar tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vectors is 10000 and each has 300 dimensions.\n"
     ]
    }
   ],
   "source": [
    "N_VECS = len(all_tweets)\n",
    "N_DIMS = len(ind2Tweet[1])\n",
    "print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing the number of planes**  \n",
    "Each plane divides the space to  2  parts, so  𝑛  planes divide the space into  2𝑛  hash buckets.\n",
    "We want to organize 10,000 document vectors into buckets so that every bucket has about 16 vectors, for that we need 10000/16=625 buckets.\n",
    "We're interested in 𝑛 , number of planes, so that 2𝑛=625. Now, we can calculate 𝑛=log2(625)=9.29≈10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PLANES = 10\n",
    "N_UNIVERSES = 25 # Number of times to repeat the hashing to improve the search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Getting the hash number for a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing hash buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes_l = [np.random.normal(size=(N_DIMS, N_PLANES)) for _ in range(N_UNIVERSES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a hash for a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining hash_value_of_vector function**\n",
    "\n",
    "**Inputs**:\n",
    "- *v*: vector of tweet. It's dimension is (1, N_DIMS)\n",
    "- *planes*: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n",
    "\n",
    "\n",
    "**Outputs**:\n",
    "- *res*: a number which is used as a hash for the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value_of_vector(v, planes):\n",
    "\n",
    "    dot_product = np.dot(v,planes)\n",
    "    sign_of_dot_product = np.sign(dot_product)\n",
    "\n",
    "    h = (sign_of_dot_product >= 0)\n",
    "    h = np.squeeze(h)\n",
    "\n",
    "    hash_value = 0\n",
    "\n",
    "    n_planes = planes.shape[1]\n",
    "    for i in range(n_planes):\n",
    "        hash_value += (2**i)*h[i]\n",
    "\n",
    "    hash_value = int(hash_value)\n",
    "\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a hash table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining make_hash_table function**\n",
    "\n",
    "**Inputs**:\n",
    "- *vecs*: list of vectors to be hashed.\n",
    "- *planes*: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n",
    "\n",
    "**Outputs**:\n",
    "- *hash_table*: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n",
    "- *id_table*: dictionary - keys are hashes, values are list of vectors id's (it's used to know which tweet corresponds to the hashed vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hash_table(vecs, planes):\n",
    "\n",
    "    num_of_planes = np.shape(planes)[1]\n",
    "    num_buckets = 2**num_of_planes\n",
    "\n",
    "    hash_table = {i:[] for i in range(num_buckets)}\n",
    "    id_table = {i:[] for i in range(num_buckets)}\n",
    "\n",
    "    for i, v in enumerate(vecs):\n",
    "        h = hash_value_of_vector(v, planes)\n",
    "        hash_table[h].append(v)\n",
    "        id_table[h].append(i)\n",
    "\n",
    "    return hash_table, id_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating all hash tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on hash universe #: 0\n",
      "working on hash universe #: 1\n",
      "working on hash universe #: 2\n",
      "working on hash universe #: 3\n",
      "working on hash universe #: 4\n",
      "working on hash universe #: 5\n",
      "working on hash universe #: 6\n",
      "working on hash universe #: 7\n",
      "working on hash universe #: 8\n",
      "working on hash universe #: 9\n",
      "working on hash universe #: 10\n",
      "working on hash universe #: 11\n",
      "working on hash universe #: 12\n",
      "working on hash universe #: 13\n",
      "working on hash universe #: 14\n",
      "working on hash universe #: 15\n",
      "working on hash universe #: 16\n",
      "working on hash universe #: 17\n",
      "working on hash universe #: 18\n",
      "working on hash universe #: 19\n",
      "working on hash universe #: 20\n",
      "working on hash universe #: 21\n",
      "working on hash universe #: 22\n",
      "working on hash universe #: 23\n",
      "working on hash universe #: 24\n"
     ]
    }
   ],
   "source": [
    "hash_tables = []\n",
    "id_tables = []\n",
    "for universe_id in range(N_UNIVERSES):\n",
    "    print('working on hash universe #:', universe_id)\n",
    "    planes = planes_l[universe_id]\n",
    "    hash_table, id_table = make_hash_table(document_vecs, planes)\n",
    "    hash_tables.append(hash_table)\n",
    "    id_tables.append(id_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approximate K-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining approximate_knn function**\n",
    "\n",
    "**Inputs**:\n",
    "- *v*: the document vector for the tweet we are willing to find neighbors.\n",
    "- *planes_l*: the list of planes (the global variable created earlier).\n",
    "- *k*: the number of nearest neighbors to search for.\n",
    "- *num_universes_to_use*: N_UNIVERSES\n",
    "\n",
    "**Outputs**:\n",
    "- *nearest_neighbor_ids*: the index of k nearest documents to the given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_knn(v, planes_l, k=1, num_universes_to_use=N_UNIVERSES):\n",
    "\n",
    "    assert num_universes_to_use <= N_UNIVERSES\n",
    "\n",
    "    vecs_to_consider_l = list()\n",
    "    ids_to_consider_l = list()\n",
    "    ids_to_consider_set = set()\n",
    "\n",
    "    for universe_id in range(num_universes_to_use):\n",
    "        planes = planes_l[universe_id]\n",
    "        hash_value = hash_value_of_vector(v, planes)\n",
    "        hash_table = hash_tables[universe_id]\n",
    "        document_vectors_l = hash_table[hash_value]\n",
    "        id_table = id_tables[universe_id]\n",
    "        new_ids_to_consider = id_table[hash_value]\n",
    "\n",
    "        for i, new_id in enumerate(new_ids_to_consider):\n",
    "\n",
    "            if new_id not in ids_to_consider_set:\n",
    "                document_vector_at_i = document_vectors_l[i]\n",
    "                vecs_to_consider_l.append(document_vector_at_i)\n",
    "                ids_to_consider_l.append(new_id)\n",
    "                ids_to_consider_set.add(new_id)\n",
    "\n",
    "    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n",
    "\n",
    "    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n",
    "\n",
    "    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n",
    "\n",
    "    nearest_neighbor_ids = [ids_to_consider_l[idx] for idx in nearest_neighbor_idx_l]\n",
    "\n",
    "    return nearest_neighbor_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using LSH to find 3 nearest neighbors to my sample tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast considering 68 vecs\n"
     ]
    }
   ],
   "source": [
    "nearest_neighbor_ids = approximate_knn(tweet_embedding, planes_l, k=3, num_universes_to_use=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document contents: i like listening to rock music!\n",
      "\n",
      "Nearest neighbor at document id 8930\n",
      "document contents: IM LISTENING TO EVERYTHING ABOUT YOU :((\n",
      "Nearest neighbor at document id 1934\n",
      "document contents: @composurs listen :-) buddy :-) u wanna play it like this :-)\n",
      "Nearest neighbor at document id 634\n",
      "document contents: @NativeMusic49 but I listen to jazz in my truck. :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Document contents: {my_tweet}\")\n",
    "print(\"\")\n",
    "\n",
    "for neighbor_id in nearest_neighbor_ids:\n",
    "    print(f\"Nearest neighbor at document id {neighbor_id}\")\n",
    "    print(f\"document contents: {all_tweets[neighbor_id]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
